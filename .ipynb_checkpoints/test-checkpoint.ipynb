{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d711b8ea-09f2-4bf0-a2c1-daf827696e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zekunl/.conda/envs/criticalmaas/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import SamProcessor\n",
    "from transformers import SamModel \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8257c8c-40ec-4e4f-816f-ff45af7b5e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ead0dc8c-f18b-445b-af21-cb812141c4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SamModel(\n",
       "  (shared_image_embedding): SamPositionalEmbedding()\n",
       "  (vision_encoder): SamVisionEncoder(\n",
       "    (patch_embed): SamPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x SamVisionLayer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): SamVisionAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): SamMLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): SamVisionNeck(\n",
       "      (conv1): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (layer_norm1): SamLayerNorm()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer_norm2): SamLayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): SamPromptEncoder(\n",
       "    (shared_embedding): SamPositionalEmbedding()\n",
       "    (mask_embed): SamMaskEmbedding(\n",
       "      (activation): GELUActivation()\n",
       "      (conv1): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv2): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv3): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (layer_norm1): SamLayerNorm()\n",
       "      (layer_norm2): SamLayerNorm()\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "    (point_embed): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): SamMaskDecoder(\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (transformer): SamTwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x SamTwoWayAttentionBlock(\n",
       "          (self_attn): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SamMLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (layer_norm4): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): SamAttention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (layer_norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (upscale_conv1): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (upscale_conv2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (upscale_layer_norm): SamLayerNorm()\n",
       "    (activation): GELU(approximate='none')\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x SamFeedForward(\n",
       "        (activation): ReLU()\n",
       "        (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (proj_out): Linear(in_features=256, out_features=32, bias=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): SamFeedForward(\n",
       "      (activation): ReLU()\n",
       "      (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (proj_out): Linear(in_features=256, out_features=4, bias=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('checkpoints_lr5e-6_hardneg/ep6.pth'))\n",
    "\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87e5e9ec-a2f5-4c56-a094-01ec72799966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get box prompt based on ground truth segmentation map\n",
    "# ground_truth_mask = np.array(dataset[idx][\"label\"])\n",
    "# prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "def test_model(img_path, model, processor):\n",
    "    \n",
    "    # img_path = '../data/data_crop1024_shift512/test_images/Greenland26X_22W_Sentinel2_2019-07-31_25_r4__h4_w2.jpg' # h3_w5 # h3_w6\n",
    "    image = Image.open(img_path)\n",
    "    \n",
    "    prompt = [[[0,0,1024,1024]]]\n",
    "    \n",
    "    # prepare image + box prompt for the model\n",
    "    inputs = processor(image, input_boxes=[[prompt]], return_tensors=\"pt\").to(device)\n",
    "    # for k,v in inputs.items():\n",
    "    #   print(k,v.shape)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, multimask_output=False)\n",
    "\n",
    "    # masks = processor.image_processor.post_process_masks(\n",
    "    #     outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu()\n",
    "    #     )\n",
    "    \n",
    "    # print(masks[0].shape)\n",
    "    \n",
    "    # apply sigmoid\n",
    "    lake_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "    lake_seg_prob = F.interpolate(lake_seg_prob, (INPUT_PATCH_SIZE, INPUT_PATCH_SIZE), mode=\"bilinear\", align_corners=False) \n",
    "    # convert soft mask to hard mask\n",
    "    lake_seg_prob = lake_seg_prob.cpu().numpy().squeeze()\n",
    "    lake_seg = (lake_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "    return lake_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "759d58f0-618b-423f-86e7-7234f0dcca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_IMG = False\n",
    "INPUT_PATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a185b9da-8d51-43c5-9d93-d47fdc105934",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/data_crop1024_shift512/test_pred_overlap_hardneg/Greenland26X_22W_Sentinel2_2019-06-03_05_r6__h21_w40.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m SAVE_IMG:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Create a Pillow image from the upscaled mask\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     upscaled_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(upscaled_mask \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m     upscaled_image\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, img_name))\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n",
      "File \u001b[0;32m~/.conda/envs/criticalmaas/lib/python3.11/site-packages/PIL/Image.py:2410\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2408\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2409\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2410\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2413\u001b[0m     save_handler(\u001b[38;5;28mself\u001b[39m, fp, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/data_crop1024_shift512/test_pred_overlap_hardneg/Greenland26X_22W_Sentinel2_2019-06-03_05_r6__h21_w40.jpg'"
     ]
    }
   ],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "# test_dir = '../data/data_crop1024_shift512/test_images_overlap/'\n",
    "# output_dir = '../data/data_crop1024_shift512/test_pred_overlap_hardneg/'\n",
    "test_dir = '../data/data_crop1024_shift512/test_images/'\n",
    "output_dir = '../data/data_crop1024_shift512/test_pred/'\n",
    "gt_dir = '../data/data_crop1024_shift512/test_mask/'\n",
    "\n",
    "img_list = os.listdir(test_dir)\n",
    "target_size = 1024\n",
    "\n",
    "for img_name in img_list[14:]:\n",
    "    img_path = os.path.join(test_dir, img_name)\n",
    "    \n",
    "    lake_seg = test_model(img_path, model, processor)\n",
    "    \n",
    "    # upscaled_mask = np.kron(lake_seg, np.ones((target_size // lake_seg.shape[0], target_size// lake_seg.shape[1])))\n",
    "\n",
    "    # # Crop to the target size\n",
    "    # upscaled_mask = upscaled_mask[:target_size, :target_size]\n",
    "\n",
    "    # upscaled_mask = upscaled_mask.astype(np.uint8)\n",
    "    # upscaled_mask = np.array(lake_seg[0][0][0])\n",
    "    upscaled_mask = lake_seg \n",
    "\n",
    "    if SAVE_IMG:\n",
    "        # Create a Pillow image from the upscaled mask\n",
    "        upscaled_image = Image.fromarray(upscaled_mask * 255)\n",
    "        upscaled_image.save(os.path.join(output_dir, img_name))\n",
    "\n",
    "    else:\n",
    "        plt.figure(figsize=(15,8))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        \n",
    "        image = Image.open(img_path)\n",
    "        # axes.imshow(image)\n",
    "        plt.imshow(image)\n",
    "    \n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(upscaled_mask)\n",
    "        # show_mask(medsam_seg, axes)\n",
    "    \n",
    "        gt_path = os.path.join(gt_dir, img_name)\n",
    "        gt_mask = Image.open(gt_path)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(gt_mask)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.show()\n",
    "        # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b01a9fce-169f-48fb-bc25-bdc66028dc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(upscaled_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e43f28ef-77cf-47d6-b19a-ec1f4e27a6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(upscaled_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08389b8b-8dd2-48fa-b1aa-359c492e869d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(upscaled_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3d3eaf-4729-4e8b-b528-a5765f720ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "criticalmaas",
   "language": "python",
   "name": "criticalmaas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
